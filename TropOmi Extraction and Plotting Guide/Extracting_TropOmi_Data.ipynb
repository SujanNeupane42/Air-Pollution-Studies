{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725ffdca",
   "metadata": {},
   "source": [
    "# Extracting TropOmi data for Nepal through EARTHDATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415a384",
   "metadata": {},
   "source": [
    "<b> This notebook will contain a complete guide to extract TropOmi data for Nepal.\n",
    "    The data is available here: [Link](https://measures.gesdisc.eosdis.nasa.gov/data/MINDS/TROPOMI_MINDS_NO2.1.1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86536524",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f06d8a",
   "metadata": {},
   "source": [
    "It is best recommended you download the data in bulk using <b> WGET</b>. After the data is downloaded, the directory structure should look like following:\n",
    "\n",
    "If the data for 2019 has been sucessfully downloaded, the directory structure will look like: \n",
    "\n",
    "    2019 -> \n",
    "        001 \n",
    "        002\n",
    "        003\n",
    "        005\n",
    "        ---\n",
    "        ---\n",
    "        364\n",
    "        365\n",
    "    Each of the subfolders inside 2019 directory will contain 14-15 .nc files containing data for each orbit. Therefore, for a given year, there will be roughly 356 * 14 files.\n",
    "    \n",
    "    Our task will be to access each subfolder inside a given year's folder, like 2019, and access Nepal's tropomi data.\n",
    "    \n",
    "    In order to do so, we will be using Python as our programming language. In addition, following libraries will also be used:\n",
    "    1. Numpy\n",
    "    2. Pandas\n",
    "    3. netCDF4\n",
    "    \n",
    "    Here, Numpy will be used to perform array manipulation. Pandas will be used to storing data in tabular format. Finally, NetCDF4 library will be utilized to read data from .nc files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf089f8c",
   "metadata": {},
   "source": [
    "# 1. Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198ec4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module will be used to access data from your local computer\n",
    "import os \n",
    "\n",
    "# This module will be used in this script to stop the program from running when a specific condition is met\n",
    "''' Like, if there is no data inside the folder. So,  sys.exit() method will stop the program execution  '''\n",
    "import sys \n",
    "\n",
    "# Datetime module is simply being used to calculate the total time taken by the program to extract the data\n",
    "from datetime import datetime\n",
    "\n",
    "# Importing the pandas library as pd alias to store and manipulate data in dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# This library will be used to access data from .nc files\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# Importing the numpy array to perform array manipulation, as our data in the .nc files will be in form of multidimensional array\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e91d27",
   "metadata": {},
   "source": [
    "# 2. Setting up the code to dynamically access specific day's data inside 2019 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1119c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an empty array to store names of .nc files that were corrupted \n",
    "files_with_errors = []\n",
    "\n",
    "# Initializing an empty array that will contain the absolute path for all .nc files present in the selected subdirectories\n",
    "path_  = []\n",
    "\n",
    "# Initializing an empty pandas dataframe that will hold the data for necessary features like Latitude, Longitude etc.\n",
    "concatenated_dataframe = pd.DataFrame({'ColumnAmountNO2Trop':[], 'Latitude': [], 'Longitude': [], 'Cloud_Fraction': [], 'QA_Value':[],\n",
    "'Date': [], 'Time': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cce9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please press enter if the python file and .nc file are in same path\n",
      "\n",
      "Enter the path to the directory where corresponding files are stored: Enter the path in your system where data exists\n",
      "Starting Folder: 1\n",
      "Ending Folder: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPlease press enter if the python file and .nc file are in same path\\n\")\n",
    "selected_directory = input(\"Enter the path to the directory where corresponding files are stored: \")\n",
    "if selected_directory == \"\":\n",
    "    selected_directory = os.getcwd()\n",
    "\n",
    "# Here, an infinite loop is created to ensure that the user will select two folders in ascending order\n",
    "''' \n",
    "Example: Start = 001, End = 100\n",
    "This is valid and Nepal's data from subfolder 001 to 100 inside 2019 folder will be extracted\n",
    "\n",
    "Howver, This is invalid: Start = 040, End = 030\n",
    "'''\n",
    "while True:\n",
    "    start = int(input('Starting Folder: '))\n",
    "    end = int(input('Ending Folder: '))\n",
    "    if start > end:\n",
    "        print('Starting folder value cannot be greater than ending folder')\n",
    "        print('please proceed again')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973016e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The next try except block of code will do the following:\n",
    "    1. From the absolute path of the directory entered by the user, where the TropOmi data exists, all folder names, path and\n",
    "       filenames will be accessed using for loop.\n",
    "    2. Using the values for start and end entered by the user in the previous step, only those subfolders inside 2019 will\n",
    "       be used whose names lie in the range of [start, end].\n",
    "    3. The subfolders which don't lie in the given range will be discarded, and the absolute path of all files inside the \n",
    "       selected subfolders will be appended to path_ list.\n",
    "    4. This will result in absolute path to .nc files from selected range of days being added to the path_ list.\n",
    "    5. Now, we will loop through each absolute path inside path_ list and exclude any files whose extension isn't .nc.\n",
    "    6. This is to ensure we don't proceed with any other file types like pdf, txt or csv. As our data is only in .nc extension.\n",
    "    7. Inside the try block, if any error occurs,  the except block will be executed.\n",
    "    8. This will result in the termination of program execution, as there is no .nc data in any of the subfolders.\n",
    "    9. This is ensured by sys.exit() method.\n",
    "       \n",
    "'''\n",
    "try:\n",
    "    for (dir_path, dir_names, file_names) in os.walk(selected_directory):\n",
    "        folder_name = os.path.basename(dir_path)\n",
    "        try:\n",
    "            if ((int(folder_name) >= start) and (int(folder_name) <= end)):\n",
    "                for name in file_names:\n",
    "                    path_.append(os.path.join(dir_path, name))\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    # this list will contain the absolute path to .nc files present in the selected subfolders\n",
    "    selected_files_path = [filename for filename in  path_ if filename.endswith('.nc')]\n",
    "    # this list will contain the names of .nc files present in the selected subfolders \n",
    "    selected_files = [file.split('\\\\')[-1] for file in selected_files_path]\n",
    "except:\n",
    "    print('\\nSorry, there are no detected files in the given path')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if len(selected_files) == 0:\n",
    "    print('\\nSorry, there are no detected files in the given path')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda7d00",
   "metadata": {},
   "source": [
    "# 3. Starting the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the total time taken for data extraction will be recorded after the next line of code executes\n",
    "startTime = datetime.now()\n",
    "\n",
    "\n",
    "''' Using the for loop, we will access the names and absolute path to selected .nc files '''\n",
    "for filepath, filename in zip(selected_files_path, selected_files):\n",
    "        # if there is some problem while reading a specific nc file, like file corruption, this try block will raise an error.\n",
    "        # And, the except block is executed which will record the names of the files.\n",
    "        try:\n",
    "            print(\"\\nSelected filename: \", filename)\n",
    "            \n",
    "            # Reading the .nc file \n",
    "            fh = Dataset(filepath, mode='r')\n",
    "\n",
    "            ''' Selecting corresponding variables'''\n",
    "            ''' For the values that are masked in the original file, I have replaced them with -1\n",
    "                so they can be easily identified later'''\n",
    "            \n",
    "            ''' Inside each nc file, the necessary variables are present inside individual groups: SCIENCE_DATA, GEOLOCATION_DATA\n",
    "                and ANCILLARY_DATA. Values with mask set to True in each variable is changed to -1, which will be removed later.\n",
    "            '''\n",
    "            \n",
    "            # Accessing the ColumnAmountNO2Trop variable\n",
    "            no2 = fh.groups['SCIENCE_DATA']['ColumnAmountNO2Trop'][:].filled(-1.)\n",
    "            no2 = no2.astype('float64')\n",
    "            \n",
    "            # Accessing the latitude variable\n",
    "            latitude = fh.groups['GEOLOCATION_DATA']['Latitude'][:].filled(-1.)\n",
    "            latitude = latitude.astype('float64')\n",
    "            \n",
    "            # Accessing the Longitude variable\n",
    "\n",
    "            longitude = fh.groups['GEOLOCATION_DATA']['Longitude'][:].filled(-1.)\n",
    "            longitude = longitude.astype('float64')\n",
    "            \n",
    "            # Accessing the cloud_fraction variable\n",
    "            cloud_fraction = fh.groups['ANCILLARY_DATA']['CloudRadianceFraction'][:].filled(-1.)\n",
    "            cloud_fraction = cloud_fraction.astype('float64')\n",
    "\n",
    "            # Accessing the qa_value\n",
    "            qa_value = fh.groups['SCIENCE_DATA']['qa_value'][:].filled(-1.)\n",
    "            qa_value = qa_value.astype('float64')\n",
    "            \n",
    "            # accessing the lat corner (4 latitude values for a given data)\n",
    "            lat_corner = fh.groups['GEOLOCATION_DATA']['CornerLatitude'][:].data\n",
    "            lat_corner = lat_corner.astype('float64')\n",
    "            \n",
    "            # Accessing the lon corner (4 longitude values for a given data)\n",
    "            lon_corner = fh.groups['GEOLOCATION_DATA']['CornerLongitude'][:].data\n",
    "            lon_corner = lon_corner.astype('float64')\n",
    "\n",
    "            ''' Closing the file'''\n",
    "            # It is best practice to close a file after reading\n",
    "            fh.close()\n",
    "\n",
    "            ''' converting the 2-d array into 1-d array '''\n",
    "            no2 = np.array(np.meshgrid(no2)[0], dtype =  np.float64)\n",
    "            latitude = np.array(np.meshgrid(latitude)[0], dtype =  np.float64)\n",
    "            longitude = np.array(np.meshgrid(longitude)[0], dtype =  np.float64)\n",
    "            cloud_fraction = np.array(np.meshgrid(cloud_fraction)[0], dtype =  np.float64)\n",
    "            qa_value = np.array(np.meshgrid(qa_value)[0], dtype =  np.float64)\n",
    "            \n",
    "            # changing the shapes of our arrays\n",
    "            ''' You can access this link to understand how np,reshape works: \n",
    "                w3schools.com/python/numpy/numpy_array_reshape.asp\n",
    "            '''\n",
    "            lat_corner = lat_corner.reshape(len(no2), 4)\n",
    "            lon_corner = lon_corner.reshape(len(no2), 4)\n",
    "            \n",
    "            # accessing 8 new variables that contain the 4 pairs of lat-lon for each data\n",
    "            LAT1 = lat_corner[:, 0]\n",
    "            LAT2 = lat_corner[:, 1]\n",
    "            LAT3 = lat_corner[:, 2]\n",
    "            LAT4 = lat_corner[:, 3]\n",
    "            LON1 = lon_corner[:, 0]\n",
    "            LON2 = lon_corner[:, 1]\n",
    "            LON3 = lon_corner[:, 2]\n",
    "            LON4 = lon_corner[:, 3]\n",
    "\n",
    "            ''' creating the dataframe'''\n",
    "            # Accessing the date and time for the data from the filename\n",
    "            date = filename[33:37]+'-'+filename[38:40]+'-'+filename[40:42]\n",
    "            time = filename[43:45]+'-'+filename[45:47]+'-'+filename[47:49]\n",
    "            df = pd.DataFrame({\n",
    "                'ColumnAmountNO2Trop':no2,\n",
    "                'Latitude':latitude,\n",
    "                'Longitude':longitude,\n",
    "                'Cloud_Fraction':cloud_fraction,\n",
    "                'QA_Value':qa_value,\n",
    "                'Date':[date for i in no2],\n",
    "                'Time':[time for i in no2],\n",
    "                'LAT1':LAT1,\n",
    "                'LAT2':LAT2,\n",
    "                'LAT3':LAT3,\n",
    "                'LAT4':LAT4,\n",
    "                'LON1':LON1,\n",
    "                'LON2':LON2,\n",
    "                'LON3':LON3,\n",
    "                'LON4':LON4\n",
    "            })\n",
    "            \n",
    "            # excluding the masked values from dataframe i.e -1\n",
    "            df = df[(df.ColumnAmountNO2Trop != -1.) & (df.Latitude != -1.)\\\n",
    "                & (df.Longitude != -1.) & (df.Cloud_Fraction != -1.) & (df.QA_Value != -1.)].reset_index(drop = True)\n",
    "\n",
    "            # only selecting the data points where latitude and longitude contain the data points for nepal\n",
    "            # these pair of latitude and longitude also contain data from tibet, bihar and possibly uttarakhand\n",
    "            # since there are roughly 2 million records in the original dataset, checking if cordinates would lie within bounding cordinates  of nepal would be complex\n",
    "            # in terms of time\n",
    "            # so, I chose these cordinates to filter out the data \n",
    "\n",
    "            df = df[(df.Latitude >= 0.0) & (df.Latitude <= 40.0) & (df.Longitude >= 60.0) & \\\n",
    "                (df.Longitude <= 100.0)].reset_index(drop = True)\n",
    "\n",
    "            if len(df) == 0:\n",
    "                print('\\nSorry, there is no data associated to ASIA on the file: '+filename)\n",
    "            else:\n",
    "                # Finally, concatenating the local datafame to our global dataframe\n",
    "                concatenated_dataframe = pd.concat([concatenated_dataframe, df]) \n",
    "        except:\n",
    "            ''' If any file raised an error while reading, this block of code executes. Therefore, the names of files with \n",
    "                errors are recorded'''\n",
    "            print('Filename has errors: ', filename)\n",
    "            files_with_errors.append(filename)\n",
    "    if len(concatenated_dataframe) == 0:\n",
    "        print(\"There is no data of nepal after concatenation.\")\n",
    "    else:\n",
    "        \n",
    "        # converting the string date column to datetime object\n",
    "        concatenated_dataframe['Date'] = pd.to_datetime(concatenated_dataframe['Date'])\n",
    "        \n",
    "        # Accessing year-month-day of the oldest data \n",
    "        least_recent_date = str(concatenated_dataframe['Date'].min())[:10]\n",
    "        \n",
    "        # Accessing year-month-day of the latest data \n",
    "        most_recent_date = str(concatenated_dataframe['Date'].max())[:10]\n",
    "\n",
    "\n",
    "        # saving the data in parquet format\n",
    "        complete_name = least_recent_date+'_'+most_recent_date\n",
    "        concatenated_dataframe.to_parquet(complete_name+'.parquet', index = True)\n",
    "        \n",
    "        # if files_with_errors list has length >0, this indicates several files raised error while reading\n",
    "        # Therefore, the names of files with errors are saved as csv files.\n",
    "        if len(files_with_errors) > 0:\n",
    "            pd.DataFrame({'Filename':files_with_errors}).to_csv('files_with_Error.csv', index=True)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        # Finally, the path where the extracted data is saved is displayed to user\n",
    "        print(\"The concatenated data is saved as: \", complete_name)\n",
    "        print(\"\\nAll corresponding files have been saved to following path: \", os. getcwd())\n",
    "        print(\"Thank you\")\n",
    "        \n",
    "        # The total time taken for data extraction is also displayed to user\n",
    "        print('\\nTotal time taken: ', str(datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ad0f7",
   "metadata": {},
   "source": [
    "# 4. Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06214be",
   "metadata": {},
   "source": [
    "<b>\n",
    "    \n",
    "    1. For a given year, data for Nepal is extracted and saved as parquet format.\n",
    "\n",
    "    2. The data for Nepal is subset of the extracted data, as only those data points were appended where lat and lon were in            range [25, 32, 80 ,88]. \n",
    "   \n",
    "    3. Therefore, for timeseries plots, the extracted data needs to be cleaned to get only Nepal's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
